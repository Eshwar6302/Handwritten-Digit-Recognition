{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "gMtJGOshF3cd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load MNIST dataset\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Normalize input data (scale pixel values to range 0-1)\n",
        "X_train = X_train.reshape(X_train.shape[0], 784) / 255.0  # Flatten images\n",
        "X_test = X_test.reshape(X_test.shape[0], 784) / 255.0\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "y_train = np.eye(10)[y_train]\n",
        "y_test = np.eye(10)[y_test]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1dnKNDTGzfR",
        "outputId": "5760efed-0a9a-4ae4-a8f4-349a0c4aea0f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return np.where(x > 0, 1, 0)\n"
      ],
      "metadata": {
        "id": "-p3S__7-G50I"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP:\n",
        "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
        "        np.random.seed(42)  # For reproducibility\n",
        "\n",
        "        # Initialize weights & biases\n",
        "        self.weights_input_hidden1 = np.random.randn(input_size, hidden_size1) * 0.01\n",
        "        self.weights_hidden1_hidden2 = np.random.randn(hidden_size1, hidden_size2) * 0.01\n",
        "        self.weights_hidden2_output = np.random.randn(hidden_size2, output_size) * 0.01\n",
        "        self.bias_hidden1 = np.zeros((1, hidden_size1))\n",
        "        self.bias_hidden2 = np.zeros((1, hidden_size2))\n",
        "        self.bias_output = np.zeros((1, output_size))\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\" Forward pass through the network \"\"\"\n",
        "        self.input = X\n",
        "        self.hidden1 = relu(np.dot(self.input, self.weights_input_hidden1) + self.bias_hidden1)\n",
        "        self.hidden2 = relu(np.dot(self.hidden1, self.weights_hidden1_hidden2) + self.bias_hidden2)\n",
        "        self.output = sigmoid(np.dot(self.hidden2, self.weights_hidden2_output) + self.bias_output)\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, X, y, learning_rate):\n",
        "        \"\"\" Backpropagation to update weights \"\"\"\n",
        "        # Compute error\n",
        "        output_error = y - self.output\n",
        "        output_delta = output_error * sigmoid_derivative(self.output)\n",
        "\n",
        "        # Hidden layer 2 error\n",
        "        hidden2_error = output_delta.dot(self.weights_hidden2_output.T)\n",
        "        hidden2_delta = hidden2_error * relu_derivative(self.hidden2)\n",
        "\n",
        "        # Hidden layer 1 error\n",
        "        hidden1_error = hidden2_delta.dot(self.weights_hidden1_hidden2.T)\n",
        "        hidden1_delta = hidden1_error * relu_derivative(self.hidden1)\n",
        "\n",
        "        # Update weights & biases\n",
        "        self.weights_hidden2_output += self.hidden2.T.dot(output_delta) * learning_rate\n",
        "        self.bias_output += np.sum(output_delta, axis=0, keepdims=True) * learning_rate\n",
        "        self.weights_hidden1_hidden2 += self.hidden1.T.dot(hidden2_delta) * learning_rate\n",
        "        self.bias_hidden2 += np.sum(hidden2_delta, axis=0, keepdims=True) * learning_rate\n",
        "        self.weights_input_hidden1 += self.input.T.dot(hidden1_delta) * learning_rate\n",
        "        self.bias_hidden1 += np.sum(hidden1_delta, axis=0, keepdims=True) * learning_rate\n",
        "\n",
        "    def train(self, X, y, epochs, learning_rate):\n",
        "        \"\"\" Train the model \"\"\"\n",
        "        loss_history = []\n",
        "        for epoch in range(epochs):\n",
        "            self.forward(X)\n",
        "            self.backward(X, y, learning_rate)\n",
        "            loss = np.mean(np.square(y - self.output))  # MSE loss\n",
        "            loss_history.append(loss)\n",
        "            if epoch % 100 == 0:\n",
        "                print(f'Epoch {epoch}, Loss: {loss}')\n",
        "        return loss_history\n"
      ],
      "metadata": {
        "id": "XYjnwY9EG9c2"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define network structure\n",
        "input_size = 784       # 28x28 pixels\n",
        "hidden_size1 = 128     # First hidden layer\n",
        "hidden_size2 = 64      # Second hidden layer\n",
        "output_size = 10       # Digits 0-9 (10 classes)\n",
        "\n",
        "# Initialize model\n",
        "mlp = MLP(input_size, hidden_size1, hidden_size2, output_size)\n",
        "\n",
        "# Train the model\n",
        "loss_history = mlp.train(X_train, y_train, epochs=1000, learning_rate=0.01)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1q4xxAzbHBQE",
        "outputId": "3bb55431-963b-43fc-88ae-1b9e838b62cb"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.25004968086222673\n",
            "Epoch 100, Loss: 0.1\n",
            "Epoch 200, Loss: 0.1\n",
            "Epoch 300, Loss: 0.1\n",
            "Epoch 400, Loss: 0.1\n",
            "Epoch 500, Loss: 0.1\n",
            "Epoch 600, Loss: 0.1\n",
            "Epoch 700, Loss: 0.1\n",
            "Epoch 800, Loss: 0.1\n",
            "Epoch 900, Loss: 0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "p6uVWGWnNS0o"
      },
      "execution_count": 43,
      "outputs": []
    }
  ]
}